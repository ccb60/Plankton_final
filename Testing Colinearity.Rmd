---
title: "Mixed Effects Linear Models to Analyze Plankton Comunity Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "6/16/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 4
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:100px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
This notebook reprises relevant analyses presented in the "Fish-GAMs.pdf"
but using mixed model linear models instead of GAMs.  The goal is to lower 
complexity of models, and gain access to some tools for automated model
selection.  

In practice, what we found using GAM analyses was that we seldom fit
relationships that were not linear between predictors and response, so we gained
little benefit from the added model complexity of using GAM models.

## A Note on Degrees of Freedom and "Singular" models
We have just under 60 complete cases available, and as our models grow
increasingly complex, we burn up degrees of freedom. The "full" linear mixed
effects model used here has the following degrees of freedom:

Source                  | Degrees of Freedom
------------------------|---------------
Intercept               | 1
Year (Random)           | 1 *
Sample Day (Random)     | 1 *
Station                 | 3
Season                  | 2
is_sp_up                | 1
Temp                    | 1
Sal                     | 1
log(Turb)               | 1
log(Chl)                | 1
log1p(Fish)             | 1
*Total*                 | *14*

Adding an interaction term between Station and Season (instead of fitting the 
"is_sp_up" term) adds $3 \times 2 - 1 = 5$ further degrees of freedom.  (The
"GAM" models we tested use additional degrees of freedom to estimate 
non-linear fits). These are complex models for a fairly small data set.

Correlations among predictors are  fairly high, so despite nominally having 
almost 60 samples, and thus on the order of 40 to 45 degrees of freedom for
error in our linear models, in practice several models return singular model 
fits. More seriously, those correlations mean different predictors 'confound'
each other.  Values (and sometimes even the sign) of model parameters are 
dependent on which other terms are retained in each model, an example of 
"Simpson's Paradox". 

While we can address the problem of confounding more formally through path 
models, an alternative, pursued here, is to use stepwise elimination of model
terms to search for minimal models that provide good predictive capability.
Stepwise methods, however, can be misled by confounding, so even this strategy 
has its pitfalls.

# Load Libraries
```{r libraries}
library(mgcv)
library(lmerTest)  # Automatically loads lme4
library(tidyverse)
library(readxl)
library(car)
library(fmsb)
library(emmeans)   # For extracting useful "marginal" model summaries
```

# Set Graphics Theme
This sets `ggplot()`graphics for no background, no grid lines, etc. in a clean
format suitable for (some) publications.
```{r set_theme}
theme_set(theme_classic())
```

# Input Data
## Folder References
```{r folder_refs}
data_folder <- "Original_Data"
```

## Load Data
```{r load_enviro_data}
filename.in <- "penob.station.data EA 3.12.20.xlsx"
file_path <- file.path(data_folder, filename.in)
station_data <- read_excel(file_path, 
                           sheet="Final", col_types = c("skip", "date", 
                                              "numeric", "text", "numeric", 
                                              "text", "skip", "skip", 
                                              "skip", 
                                              rep("numeric", 10),
                                              "text", 
                                              rep("numeric", 47),
                                              "text",
                                              rep("numeric", 12))) %>%
  rename_with(~ gsub(" ", "_", .x)) %>%
  rename_with(~ gsub("\\.", "_", .x)) %>%
  rename_with(~ gsub("\\?", "", .x)) %>%
  rename_with(~ gsub("%", "pct", .x)) %>%
  rename_with(~ gsub("_Abundance", "", .x)) %>%
  filter(! is.na(date))
```

Station names are arbitrary, and Erin previously expressed interest in renaming 
them from Stations 2, 4, 5 and 8 to Stations 1,2,3,and 4.

The `factor()` function by default sorts levels before assigning numeric codes,
so a convenient way to replace the existing station codes with sequential
numbers is to create a factor and extract the numeric indicator values with 
`as.numeric()`.

```{r change_station_names_2}
station_data <- station_data %>%
  mutate(station = factor(as.numeric(factor(station))))
head(station_data)
```

### Subsetting to Desired Data Columns
I base selection of predictor variables here on the ones used in the manuscript.

```{r build_env_data}
base_data <- station_data %>%
  rename(Date = date, 
         Station = station,
         Year = year) %>%
  select(-c(month, month_num)) %>%
  mutate(Month = factor(as.numeric(format(Date, format = '%m')),
                                                levels = 1:12, 
                                                labels = month.abb),
         DOY = as.numeric(format(Date,format = '%j')),
         season = factor(season, levels = c('Spring', 'Summer', 'Fall')),
         is_sp_up = season == 'Spring' & Station == 1,
         Yearf = factor(Year)) %>%
  rename(Season = season,
         Temp = ave_temp_c,
         Sal = ave_sal_psu,
         Turb = sur_turb,
         AvgTurb = ave_turb_ntu,
         DOsat = ave_DO_Saturation,
         Chl = ave_chl_microgperl,
         Fish = `___61`,
         RH = Herring
         ) %>%
  select(Date, Station, Year, Yearf, Month, Season, is_sp_up, DOY, riv_km, 
         Temp, Sal, Turb, AvgTurb, DOsat, Chl, 
         Fish, RH, 
         combined_density,H, SEI,
         Acartia, Balanus, Eurytemora, Polychaete, Pseudocal, Temora) %>%
  arrange(Date, Station)
head(base_data)
```

```{r}
rm(station_data)
```

### Add Transformed Predictors
We can treat the sampling history as "spring", "summer" and "fall" observations 
each year from 2013 through 2017.  This breaks the temporal pattern down 
into integer valued time, generating a "quasi regular" time series, and
allowing us to simplify the analysis of temporal autocorrelation.  The "real 
world" time difference across the winter is longer that between seasons, but  I
could not find a ready way to address that.

We need both the numerical sequence and a factor later, for different purposes.

```{r}
base_data <- base_data %>%
  mutate(sample_seq = as.numeric(Season) + (Year-2013)*3,
         sample_event = factor(sample_seq))
```

## Complete Cases
This drops only two samples, one for missing Zooplankton data, one for missing
fish data.  We need this reduced data set to run The `step()` function. It
makes little sense to try stepwise model selection if each time you add or 
remove a variable, the sample you are studying changes.  Since fish is never an
important predictor, we may want need to refit models after stepwise elimination
to use the most complete possible data set.

```{r}
complete_data <- base_data %>%
  select(Season, Station, Yearf, sample_event, 
         is_sp_up, Temp, Sal, Turb, Chl, Fish, RH,
         combined_density, H, 
         Acartia, Balanus, Eurytemora, Polychaete, Pseudocal, Temora) %>%
  filter(complete.cases(.))
```

# LMER Model of Fish Abundance
```{r}
fish_lmer <- lmer(log1p(Fish) ~ Station * Season +
                     Temp +
                     Sal + 
                     log(Turb) + 
                     log(Chl) + 
                     log1p(combined_density) + 
                     (1 | Yearf) + (1 | Yearf:sample_event),
                   data = base_data, na.action = na.omit, REML = FALSE)
#summary(fish_lmer)
```


```{r}
vif(fish_lmer)
```

So, this model shows relatively mild VIF for MOST predictor variables. However,
Temperature has a high VIF, over 10, ans Salinity aproaches the "cutoff" value
of 5.  We know from prior work that both are highly correlated with season and 
station.

We can look at several modeling strategies, but note that these are precisely 
the terms that are most important in our model.  We have a problem here....


```{r}
anova(fish_lmer)
```

In the Full Model, Salinity appears as the sole statistically important fixed 
effect, but Season and temperature are not far from significant.

## Stepwise Model Selection
The `lmerTest` package includes a backward elimination algorithm that first
searches for random effects that provide little explanatory power (by likelihood
ratio test), then for fixed effects that can be dropped.
```{r}
(fish_step_0 <- step(fish_lmer, 
                   #reduce.random = FALSE,  # add to not drop random terms
                   #keep = c('Station', 'Season')
                   ))
fish_step_0 <- get_model(fish_step_0)
```

```{r}
anova(fish_step_0)
```

Stepwise elimination can change what appears to be important in the model.
Salinity is correlated with eliminated model terms, especially Station, so 
if you eliminate Station from the model, the apparent importance of Temperature
and Salinity changes.  One way to think about this is to say that given the 
Station you are sampling, salinity matters, but if you don't know where you are 
in the estuary, the effect of Salinity gets swamped out by differences between 
locations. This is a great example of Simpson's Paradox. Dropping Station
from the model here probably gives misleading results.

I run the stepwise model selection again, specify that we want to keep the 
"experimental variables" (Station and Season) in the model. I also shift the
threshold for dropping a fixed effect from P> 0.05 to P> 0.10, to make this
step more conservative.

```{r}
(fish_step_1 <- step(fish_lmer, alpha.fixed = 0.1,
                   #reduce.random = FALSE,  # add to not drop random terms
                   keep = c('Station', 'Season')
                   ))
fish_step_1 <- get_model(fish_step_1)
```

```{r}
anova(fish_step_1)
```

Note that this model has four more parameters, but reduces the residual sums of 
squares and and mean square slightly.

Now Salinity is retained as an important variable, and temperature is 
retained because it is marginally significant.  Both of our "random" factors 
were still dropped.  

Dropping the random factors may also not be especially appropriate, if we
believe that year and Sample Event represent important groups of correlated
observations. Dropping `Yearf`, in particular, may also risk Simpson's Paradox,
since effects of other predictors could be swamped out if year to year 
variation is sufficiently large.

The case for retaining `Yearf` on first principals is pretty strong, 
regardless of whether  the random term is important by likelihood ratio test
or not. The case for Sample_Event, less so.  Here, I simply prevent `step()` 
from removing the two random terms.

```{r}
(fish_step_2 <- step(fish_lmer, alpha.fixed = 0.1,
                   reduce.random = FALSE,  # add to not drop random terms
                   keep = c('Station', 'Season')
                   ))
fish_step_2 <- get_model(fish_step_2)
```

```{r}
anova(fish_step_2)
```

And now TEMPERATURE drops out of the model, but both Season and Station now 
look important.  This model produces results that are qualitatively similar to
the GAM model that includes both random factors.

```{r}
AIC(fish_step_0, fish_step_1, fish_step_2)

```

By AIC, the "best" model is the simplest, which should be no surprise, since
it was returned by the stepwise elimination procedure. But hte differences are
relativley minor.  And I worry about invoking Simpson's paradox if we don't 
condition responses by the experimental variables.

Let's check the VIFs....

```{r}
vif(fish_step_2)
```

Notice that we now have no high variance inflation factors.

###  Some Model Diagnostics
```{r}
plot(fish_step_2, resid(., type = "pearson") ~ fitted(.), abline = 0, 
     id = 0.05)
plot(fish_step_2, sqrt(abs(resid(., type = "pearson"))) ~ fitted(.))
plot(fish_step_2, hatvalues(.) ~ fitted(.))
as_tibble(resid(fish_step_2)) %>% ggplot(aes(value)) + geom_histogram(bins = 20)
```

So, residuals are slightly heavy tailed, but no outliers have very high
leverage, so this is probably a pretty good model, so long as we don't take our
P values too seriously.

# GAMM Model of Fish Abundance
```{r}
fish_gam <- gamm(log1p(Fish) ~ Station + 
                     Season +
                     s(Temp, bs="ts") +
                     s(Sal, bs="ts") + 
                     s(log(Turb), bs="ts") + 
                     s(log(Chl), bs="ts") + 
                     s(log1p(combined_density),bs="ts"),
                   random = list(Yearf = ~ 1, sample_event = ~ 1),
                   data = base_data, family = 'gaussian')
anova(fish_gam$gam)
```

```{r}
concurvity(fish_gam$gam)
```


I don't really understand the output. Values near 1 are supposedly bad.  
Everything is near 1. Especially Temperature.  So I drop the clearly unimportant 
factors and Temp, and see where we are...


```{r}
fish_gam_2 <- gamm(log1p(Fish) ~ Station + 
                     Season +
                     s(Sal, bs="ts") + 
                     s(log1p(combined_density),bs="ts"),
                   random = list(Yearf = ~ 1, sample_event = ~ 1),
                   data = base_data, family = 'gaussian')
anova(fish_gam_2$gam)
```

```{r}
concurvity(fish_gam_2$gam)
```

Note that the measures of concurvidity are much lower. Al lare below a "threshold"
of 0.8.  I have no idea where that threshold of 0.8 came from, but I found it a couple of places on-line....

But we can still simplify the model further by omitting the plankton terms.

```{r}
fish_gam_3 <- gamm(log1p(Fish) ~ Station + 
                     Season +
                     s(Sal, bs="ts"),
                   random = list(Yearf = ~ 1, sample_event = ~ 1),
                   data = base_data, family = 'gaussian')
anova(fish_gam_3$gam)
```
```{r}
concurvity(fish_gam_3$gam)
```

That lowers concurvity values somewhat more.

Note that Season is Significant, Salinity is Significant, and Station is ALMOST significant.

```{r}
plot(fish_gam_3$gam)
```
It fits a linear model....  So there is no reason this should differ in any 
substantive way from an LMER analysis, and in fact that analysis gets us to 
exactly the same place.


# Model of Total Zooplankton Density
```{r}
density_lmer <- lmer(log(combined_density) ~ 
                          Station *  
                          Season +
                          #is_sp_up +
                          Temp +
                          Sal + 
                          log(Turb) + 
                          log(Chl) + 
                          log1p(Fish) +
                          (1 | Yearf) + (1 | sample_event),
                        data = complete_data, na.action = na.omit)
anova(density_lmer)
```

Salinity, Turbidity, and Chlorophyll are significant predictors, as are Season 
and the Station by Season interaction term. We know from prior analysis that the 
interaction is because things work differently in those spring upstream samples.

## Stepwise Model Selection
The `lmerTest` package includes a backward elimination algorithm that first
searches for random effects that provide little explanatory power (by likelihood
ratio test), then for fixed effects that can be dropped.
```{r}
(density_step <- step(density_lmer, 
                      #reduce.random = FALSE,  # add to not drop random terms
                   keep = c('Station', 'Season')
                   ))
density_step <- get_model(density_step)
```

So, this stepwise process retains a random effect for Sample Event, and
otherwise does little to simplify the model, dropping only fish abundance as 
a predictor.  We refit with `is_sp_up` instead of a full Season by Station 
interaction, and with many more interaction terms, to test if they matter.

```{r}
density_lmer_2 <- lmer(log(combined_density) ~ 
                          is_sp_up + (Station  +  Season) *
                          (Temp +
                          Sal + 
                          log(Turb) + 
                          log(Chl)) + 
                          (1 | sample_event),
                        data = base_data, na.action = na.omit)
anova(density_lmer_2)
```

We see that many interactions ARE important.  lets ru `step()` again to 
simplify this model as much as possible.

```{r}
(density_step_2 <- step(density_lmer_2, 
                   keep = c('Station', 'Season')
                   ))
density_step_2 <- get_model(density_step_2)
```
Interestingly, in a complicated model where I allow lots of other interactions,
the "different" behavior of upstream spring samples disappears


###  Some Model Diagnostics
```{r fig.width = 3, fig.height = 2)}
plot(density_step_2, resid(., type = "pearson") ~ fitted(.), abline = 0, 
     id = 0.05)
plot(density_step_2, sqrt(abs(resid(., type = "pearson"))) ~ fitted(.))
plot(density_step_2, hatvalues(.) ~ fitted(.))
as_tibble(resid(density_step_2)) %>% ggplot(aes(value)) + geom_histogram(bins = 20)
```

### Dealing with High Leverage Points
We still have one big outlier, and a couple of other high leverage points.
These samples are well fit by the existing model -- precisely because they are
fairly high leverage. We can check to see which samples they are as follows:

```{r}
outlier <- which(predict(density_step_2) < 5)
leverage <- which(hatvalues(density_step_2) > 0.75 )
base_data[leverage,]
```


```{r fig.height = 2, fig.width = 5}
base_data %>%
  ggplot(aes(Sal)) + 
  geom_histogram(aes(fill = (hatvalues(density_step_2) > 0.75 )), bins = 20)
```

The high leverage points are one of the spring "washout" samples and two spring 
samples from 2013. It is not immediately obvious why those two 2013 spring 
samples have such high leverage.

We have several potential ways forward -- drop the outlier, drop all low
salinity samples, or drop high leverage points. I don't like dropping high
leverage points, as that feels very *ad hoc*. I don't much like droppinig 
outliers either.  Either one effectively means you are dropping data because it
does not fit your model, which feels wrong-headed to me. Data is supposed to 
inform your model, not the other way around.  

I am a bit more comfortable with restricting analysis based on a describable 
feature of the data, such as dropping all low salinity samples. That at least
delineates what a  revised model CAN'T do: answer questions about low salinity 
samples.

## Revised Model
```{r}
tmp <- base_data[! base_data$Sal < 5,]
density_lmer_3 <- lmer(log(combined_density) ~ 
                          (Station  +  Season) *
                          (Temp +
                          Sal + 
                          log(Turb) + 
                          log(Chl)) + 
                          Station:Season +
                          (1 | sample_event),
                        data = tmp, na.action = na.omit)
(density_step_3 <- step(density_lmer_3, 
                   keep = c('Station', 'Season')
                   ))
density_step_3<- get_model(density_step_3)
```

```{r}
anova(density_step_3)
```

```{r}
summary(density_step_3)
```

###  More  Model Diagnostics
```{r fig.width = 3, fig.height = 2}
plot(density_step_3, resid(., type = "pearson") ~ fitted(.), abline = 0, 
     id = 0.05)
plot(density_step_3, sqrt(abs(resid(., type = "pearson"))) ~ fitted(.))
plot(density_step_3, hatvalues(.) ~ fitted(.))
plot(shannon_step_2, cooks.distance(.) ~ fitted(.))
as_tibble(resid(density_step_3)) %>% ggplot(aes(value)) + geom_histogram(bins = 20)
```
So, dropping low salinity samples has a big effect on model fit, eliminating
many of the interaction terms, and largely eliminating model pathologies. We
still have a couple of moderately high leverage points, but this model looks
more trustworthy, at the expense of not predicting our most extreme "spring
washout" samples.




